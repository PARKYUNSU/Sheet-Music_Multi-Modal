"""
ê°œì„ ëœ OMR ì‹œìŠ¤í…œ - ìŒí‘œ ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
"""

import cv2
import numpy as np
import json
from typing import List, Dict, Optional
from advanced_omr import AdvancedOMR

class ImprovedOMR(AdvancedOMR):
    def __init__(self):
        super().__init__()
        print("ê°œì„ ëœ OMR ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def preprocess_image_improved(self, image_path: str) -> np.ndarray:
        """ê°œì„ ëœ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - ì˜¤ì„  ì œê±° ë° ìŒí‘œ ê°•í™”"""
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path, cv2.IMREAD_COLOR)
        if image is None:
            raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. ì˜¤ì„  ê²€ì¶œ ë° ì œê±°
        binary_without_staff = self.remove_staff_lines(gray)
        
        # 2. ìŒí‘œ ê°•í™”
        enhanced_notes = self.enhance_notes(binary_without_staff)
        
        return enhanced_notes
    
    def remove_staff_lines(self, gray_image: np.ndarray) -> np.ndarray:
        """ì˜¤ì„  ì œê±°"""
        # ì ì‘ì  ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„í™”
        binary = cv2.adaptiveThreshold(
            gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY_INV, 11, 2
        )
        
        # ìˆ˜í‰ì„  ê²€ì¶œì„ ìœ„í•œ êµ¬ì¡° ìš”ì†Œ
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
        
        # ìˆ˜í‰ì„  ê²€ì¶œ
        horizontal_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel)
        
        # ì˜¤ì„  ì œê±° (ìˆ˜í‰ì„ ì„ í°ìƒ‰ìœ¼ë¡œ ë®ìŒ)
        result = binary.copy()
        result[horizontal_lines > 0] = 0  # ì˜¤ì„ ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ë³€ê²½
        
        return result
    
    def enhance_notes(self, binary_image: np.ndarray) -> np.ndarray:
        """ìŒí‘œ ê°•í™”"""
        # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        cleaned = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel)
        
        # ìŒí‘œ ì—°ê²° (ì ë“¤ì„ ì—°ê²°)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        enhanced = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)
        
        return enhanced
    
    def find_note_candidates_improved(self, staff_region: np.ndarray) -> List[Dict]:
        """ê°œì„ ëœ ìŒí‘œ í›„ë³´ ê²€ì¶œ"""
        # ì»¨íˆ¬ì–´ ê²€ì¶œ
        contours, _ = cv2.findContours(staff_region, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        candidates = []
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # ë” ê´€ëŒ€í•œ ë©´ì  ì¡°ê±´
            if 5 < area < 5000:  # ê¸°ì¡´ 50-1000ì—ì„œ 5-5000ìœ¼ë¡œ í™•ì¥
                x, y, w, h = cv2.boundingRect(contour)
                
                # ì¢…íš¡ë¹„ í•„í„° (ë„ˆë¬´ ê¸¸ì­‰í•œ ê²ƒ ì œì™¸)
                aspect_ratio = w / h if h > 0 else 0
                if 0.1 < aspect_ratio < 10:  # ì ì ˆí•œ ì¢…íš¡ë¹„
                    candidates.append({
                        'contour': contour,
                        'bbox': (x, y, w, h),
                        'area': area,
                        'aspect_ratio': aspect_ratio
                    })
        
        return candidates
    
    def classify_note_type_improved(self, candidate: Dict) -> str:
        """ê°œì„ ëœ ìŒí‘œ ì¢…ë¥˜ ë¶„ë¥˜"""
        area = candidate['area']
        aspect_ratio = candidate['aspect_ratio']
        x, y, w, h = candidate['bbox']
        
        # ë©´ì ê³¼ ì¢…íš¡ë¹„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜
        if area < 20:
            return 'eighth'  # 8ë¶„ìŒí‘œ
        elif area < 50:
            return 'quarter'  # 4ë¶„ìŒí‘œ
        elif area < 100:
            return 'half'  # 2ë¶„ìŒí‘œ
        else:
            return 'whole'  # ì˜¨ìŒí‘œ
    
    def detect_notes_improved(self, binary_image: np.ndarray, staff_groups: List[Dict]) -> List[Dict]:
        """ê°œì„ ëœ ìŒí‘œ ê²€ì¶œ"""
        notes = []
        
        for staff in staff_groups:
            # ì˜¤ì„  ì˜ì—­ ì¶”ì¶œ (ì—¬ìœ  ê³µê°„ í™•ëŒ€)
            staff_region = binary_image[staff['top']-30:staff['bottom']+30, :]
            
            # ê°œì„ ëœ ìŒí‘œ í›„ë³´ ê²€ì¶œ
            note_candidates = self.find_note_candidates_improved(staff_region)
            
            print(f"ì˜¤ì„  ê·¸ë£¹ì—ì„œ {len(note_candidates)}ê°œ í›„ë³´ ê²€ì¶œ")
            
            # ê° í›„ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ìŒí‘œ ì •ë³´ ì¶”ì¶œ
            for candidate in note_candidates:
                note_info = self.analyze_note_candidate_improved(candidate, staff)
                if note_info:
                    notes.append(note_info)
        
        return notes
    
    def analyze_note_candidate_improved(self, candidate: Dict, staff: Dict) -> Optional[Dict]:
        """ê°œì„ ëœ ìŒí‘œ í›„ë³´ ë¶„ì„"""
        x, y, w, h = candidate['bbox']
        
        # ê°œì„ ëœ ìŒí‘œ ì¢…ë¥˜ ë¶„ë¥˜
        note_type = self.classify_note_type_improved(candidate)
        
        # ìŒë†’ì´ ê³„ì‚° (ì˜¤ì„ ê³¼ì˜ ìƒëŒ€ì  ìœ„ì¹˜)
        pitch = self.calculate_pitch_improved(y, staff)
        
        # ìŒí‘œ ê¸¸ì´ ì¶”ì •
        duration = self.estimate_duration_improved(candidate)
        
        return {
            'x': x,
            'y': y,
            'width': w,
            'height': h,
            'type': note_type,
            'pitch': pitch,
            'duration': duration,
            'staff_index': staff.get('index', 0),
            'area': candidate['area'],
            'aspect_ratio': candidate['aspect_ratio']
        }
    
    def calculate_pitch_improved(self, y: int, staff: Dict) -> str:
        """ê°œì„ ëœ ìŒë†’ì´ ê³„ì‚°"""
        staff_lines = staff['lines']
        staff_height = staff_lines[4]['y'] - staff_lines[0]['y']
        
        # ì˜¤ì„  ê°„ê²©
        line_spacing = staff_height / 4
        
        # ìŒí‘œê°€ ì–´ëŠ ì˜¤ì„ /ê°„ì— ìˆëŠ”ì§€ ê³„ì‚°
        relative_position = (y - staff_lines[0]['y']) / line_spacing
        
        # ìŒë†’ì´ ë§¤í•‘ (ë” ì •í™•í•œ ê³„ì‚°)
        pitch_map = {
            -2: 'C4', -1.5: 'D4', -1: 'E4', -0.5: 'F4',
            0: 'G4', 0.5: 'A4', 1: 'B4', 1.5: 'C5',
            2: 'D5', 2.5: 'E5', 3: 'F5', 3.5: 'G5',
            4: 'A5', 4.5: 'B5', 5: 'C6', 5.5: 'D6',
            6: 'E6', 6.5: 'F6', 7: 'G6'
        }
        
        # ê°€ì¥ ê°€ê¹Œìš´ ìœ„ì¹˜ ì°¾ê¸°
        closest_pos = min(pitch_map.keys(), key=lambda x: abs(x - relative_position))
        return pitch_map[closest_pos]
    
    def estimate_duration_improved(self, candidate: Dict) -> float:
        """ê°œì„ ëœ ìŒí‘œ ê¸¸ì´ ì¶”ì •"""
        note_type = self.classify_note_type_improved(candidate)
        duration_map = {
            'whole': 4.0,
            'half': 2.0,
            'quarter': 1.0,
            'eighth': 0.5,
            'sixteenth': 0.25
        }
        return duration_map.get(note_type, 1.0)
    
    def process_sheet_music_improved(self, image_path: str) -> Dict:
        """ê°œì„ ëœ ì•…ë³´ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        import time
        start_time = time.time()
        
        # ì›ë³¸ ì´ë¯¸ì§€ë¡œ ì˜¤ì„  ê²€ì¶œ (ì˜¤ì„  ì œê±° ì „ì— í•´ì•¼ í•¨)
        original_image = cv2.imread(image_path, cv2.IMREAD_COLOR)
        gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)
        binary_original = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY_INV, 11, 2
        )
        
        # ì˜¤ì„  ê²€ì¶œ (ì›ë³¸ ì´ì§„í™” ì´ë¯¸ì§€ì—ì„œ)
        staff_groups = self.detect_staff_lines_advanced(binary_original)
        
        # ê°œì„ ëœ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì˜¤ì„  ì œê±°)
        binary_image = self.preprocess_image_improved(image_path)
        
        # ê°œì„ ëœ ìŒí‘œ ê²€ì¶œ
        notes = self.detect_notes_improved(binary_image, staff_groups)
        
        # ê°€ì‚¬ ì¶”ì¶œ
        lyrics = self.extract_lyrics_advanced(image_path, staff_groups)
        
        processing_time = time.time() - start_time
        
        return {
            'notes': notes,
            'lyrics': lyrics,
            'staff_groups': staff_groups,
            'processing_time': processing_time,
            'total_notes': len(notes)
        }

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_improved_omr():
    """ê°œì„ ëœ OMR ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê°œì„ ëœ OMR ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    omr = ImprovedOMR()
    result = omr.process_sheet_music_improved('sheet music_1.png')
    
    print(f"\nğŸ“Š ê²°ê³¼:")
    print(f"  - ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.3f}ì´ˆ")
    print(f"  - ê²€ì¶œëœ ìŒí‘œ ìˆ˜: {result['total_notes']}")
    print(f"  - ì˜¤ì„  ê·¸ë£¹ ìˆ˜: {len(result['staff_groups'])}")
    
    if result['notes']:
        print(f"\nğŸµ ê²€ì¶œëœ ìŒí‘œë“¤:")
        for i, note in enumerate(result['notes'][:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            print(f"  {i+1}. {note['type']} - {note['pitch']} (ë©´ì : {note['area']:.1f})")
    else:
        print("\nâŒ ìŒí‘œê°€ ê²€ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    result_serializable = convert_numpy_types(result)
    
    with open('improved_omr_result.json', 'w', encoding='utf-8') as f:
        json.dump(result_serializable, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: improved_omr_result.json")

if __name__ == "__main__":
    test_improved_omr()
