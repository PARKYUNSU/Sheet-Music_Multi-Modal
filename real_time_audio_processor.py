"""
ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ìŒë†’ì´ ê²€ì¶œ ì‹œìŠ¤í…œ
MP3 íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë¶„ì„ ìˆ˜í–‰
"""

import librosa
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import pyaudio
import threading
import time
from collections import deque
import json
from typing import List, Dict, Optional, Tuple

class RealTimeAudioProcessor:
    def __init__(self, audio_file_path: str, chunk_size: int = 1024, sample_rate: int = 22050):
        """
        ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            audio_file_path: MP3 íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸° (í”„ë ˆì„ ë‹¨ìœ„)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        """
        self.audio_file_path = audio_file_path
        self.chunk_size = chunk_size
        self.sample_rate = sample_rate
        
        # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        self.audio_data = None
        self.duration = 0
        self.load_audio_file()
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ ë³€ìˆ˜
        self.is_playing = False
        self.current_position = 0
        self.pitch_history = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ìŒë†’ì´ ì €ì¥
        self.volume_history = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ë³¼ë¥¨ ì €ì¥
        
        # PyAudio ì„¤ì •
        self.p = None
        self.stream = None
        
        # ìŒë†’ì´ ê²€ì¶œ ì„¤ì •
        self.pitch_threshold = 0.1  # ìŒë†’ì´ ê²€ì¶œ ìµœì†Œ ì„ê³„ê°’
        self.f0_min = 80.0  # ìµœì†Œ ì£¼íŒŒìˆ˜ (Hz)
        self.f0_max = 1000.0  # ìµœëŒ€ ì£¼íŒŒìˆ˜ (Hz)
        
        print(f"ğŸµ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - íŒŒì¼: {audio_file_path}")
        print(f"   - ê¸¸ì´: {self.duration:.2f}ì´ˆ")
        print(f"   - ìƒ˜í”Œë§ ë ˆì´íŠ¸: {self.sample_rate}Hz")
    
    def load_audio_file(self):
        """MP3 íŒŒì¼ ë¡œë“œ"""
        try:
            print(f"ğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì¤‘: {self.audio_file_path}")
            
            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            self.audio_data, sr = librosa.load(
                self.audio_file_path, 
                sr=self.sample_rate,
                mono=True  # ëª¨ë…¸ë¡œ ë³€í™˜
            )
            
            self.duration = len(self.audio_data) / self.sample_rate
            print(f"âœ… ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ: {len(self.audio_data)} ìƒ˜í”Œ, {self.duration:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def detect_pitch(self, audio_chunk: np.ndarray) -> Tuple[float, float]:
        """
        ìŒë†’ì´ ê²€ì¶œ (YIN ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        
        Args:
            audio_chunk: ì˜¤ë””ì˜¤ ì²­í¬
            
        Returns:
            (ìŒë†’ì´(Hz), ì‹ ë¢°ë„)
        """
        try:
            # YIN ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìŒë†’ì´ ê²€ì¶œ (threshold ë§¤ê°œë³€ìˆ˜ ì œê±°)
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio_chunk,
                fmin=self.f0_min,
                fmax=self.f0_max,
                sr=self.sample_rate
            )
            
            # ìœ íš¨í•œ ìŒë†’ì´ë§Œ ì¶”ì¶œ
            valid_f0 = f0[~np.isnan(f0)]
            if len(valid_f0) > 0:
                # í‰ê·  ìŒë†’ì´ ê³„ì‚°
                avg_f0 = np.mean(valid_f0)
                confidence = np.mean(voiced_probs[~np.isnan(voiced_probs)])
                return avg_f0, confidence
            else:
                return 0.0, 0.0
                
        except Exception as e:
            print(f"ìŒë†’ì´ ê²€ì¶œ ì˜¤ë¥˜: {e}")
            return 0.0, 0.0
    
    def detect_volume(self, audio_chunk: np.ndarray) -> float:
        """ë³¼ë¥¨ ê²€ì¶œ (RMS)"""
        return np.sqrt(np.mean(audio_chunk ** 2))
    
    def frequency_to_note(self, frequency: float) -> str:
        """ì£¼íŒŒìˆ˜ë¥¼ ìŒí‘œëª…ìœ¼ë¡œ ë³€í™˜"""
        if frequency <= 0:
            return "Silence"
        
        # A4 = 440Hzë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        A4 = 440.0
        note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        
        # ë°˜ìŒ ë‹¨ìœ„ë¡œ ë³€í™˜
        semitones = 12 * np.log2(frequency / A4)
        
        # ì˜¥íƒ€ë¸Œì™€ ìŒí‘œ ê³„ì‚°
        octave = int(4 + semitones // 12)
        note_index = int(semitones % 12)
        
        if note_index < 0:
            note_index += 12
            octave -= 1
        
        return f"{note_names[note_index]}{octave}"
    
    def process_audio_chunk(self, audio_chunk: np.ndarray) -> Dict:
        """ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬"""
        # ìŒë†’ì´ ê²€ì¶œ
        pitch, confidence = self.detect_pitch(audio_chunk)
        
        # ë³¼ë¥¨ ê²€ì¶œ
        volume = self.detect_volume(audio_chunk)
        
        # ìŒí‘œëª… ë³€í™˜
        note_name = self.frequency_to_note(pitch)
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.pitch_history.append(pitch)
        self.volume_history.append(volume)
        
        return {
            'pitch': pitch,
            'confidence': confidence,
            'volume': volume,
            'note_name': note_name,
            'timestamp': time.time()
        }
    
    def play_audio_realtime(self, callback_interval: float = 0.1):
        """
        ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì¬ìƒ ë° ë¶„ì„
        
        Args:
            callback_interval: ì½œë°± í˜¸ì¶œ ê°„ê²© (ì´ˆ)
        """
        print(f"ğŸµ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘...")
        
        # PyAudio ì´ˆê¸°í™”
        self.p = pyaudio.PyAudio()
        
        # ìŠ¤íŠ¸ë¦¼ ì„¤ì •
        self.stream = self.p.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=self.sample_rate,
            output=True,
            frames_per_buffer=self.chunk_size
        )
        
        self.is_playing = True
        self.current_position = 0
        
        # ì˜¤ë””ì˜¤ ì¬ìƒ ë° ë¶„ì„ ìŠ¤ë ˆë“œ
        def audio_thread():
            chunk_samples = int(self.chunk_size)
            callback_samples = int(self.sample_rate * callback_interval)
            
            while self.is_playing and self.current_position < len(self.audio_data):
                # í˜„ì¬ ì²­í¬ ì¶”ì¶œ
                end_pos = min(self.current_position + chunk_samples, len(self.audio_data))
                audio_chunk = self.audio_data[self.current_position:end_pos]
                
                # ì˜¤ë””ì˜¤ ì¬ìƒ
                if len(audio_chunk) > 0:
                    self.stream.write(audio_chunk.astype(np.float32).tobytes())
                
                # ì˜¤ë””ì˜¤ ë¶„ì„
                if len(audio_chunk) >= callback_samples:
                    analysis_chunk = audio_chunk[:callback_samples]
                    result = self.process_audio_chunk(analysis_chunk)
                    
                    # ê²°ê³¼ ì¶œë ¥
                    if result['pitch'] > 0:
                        print(f"ğŸµ {result['note_name']:>4} | "
                              f"ì£¼íŒŒìˆ˜: {result['pitch']:6.1f}Hz | "
                              f"ì‹ ë¢°ë„: {result['confidence']:.2f} | "
                              f"ë³¼ë¥¨: {result['volume']:.3f}")
                    else:
                        print(f"ğŸ”‡ Silence | ë³¼ë¥¨: {result['volume']:.3f}")
                
                self.current_position = end_pos
                
                # ì¬ìƒ ì†ë„ ì¡°ì ˆ
                time.sleep(callback_interval)
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        audio_thread_obj = threading.Thread(target=audio_thread)
        audio_thread_obj.start()
        
        return audio_thread_obj
    
    def stop_audio(self):
        """ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ì§€"""
        self.is_playing = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        if self.p:
            self.p.terminate()
        print("â¹ï¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ì§€")
    
    def analyze_full_audio(self) -> Dict:
        """ì „ì²´ ì˜¤ë””ì˜¤ ë¶„ì„"""
        print("ğŸ” ì „ì²´ ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œì‘...")
        
        # ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
        chunk_size = int(self.sample_rate * 0.1)  # 0.1ì´ˆ ì²­í¬
        results = []
        
        for i in range(0, len(self.audio_data), chunk_size):
            chunk = self.audio_data[i:i+chunk_size]
            if len(chunk) > 0:
                result = self.process_audio_chunk(chunk)
                result['time'] = i / self.sample_rate
                results.append(result)
        
        # í†µê³„ ê³„ì‚°
        pitches = [r['pitch'] for r in results if r['pitch'] > 0]
        volumes = [r['volume'] for r in results]
        
        analysis_result = {
            'total_duration': self.duration,
            'total_chunks': len(results),
            'pitch_statistics': {
                'min_pitch': min(pitches) if pitches else 0,
                'max_pitch': max(pitches) if pitches else 0,
                'avg_pitch': np.mean(pitches) if pitches else 0,
                'pitch_range': max(pitches) - min(pitches) if pitches else 0
            },
            'volume_statistics': {
                'min_volume': min(volumes) if volumes else 0,
                'max_volume': max(volumes) if volumes else 0,
                'avg_volume': np.mean(volumes) if volumes else 0
            },
            'detailed_results': results
        }
        
        print(f"âœ… ì „ì²´ ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ")
        print(f"   - ì´ ì²­í¬ ìˆ˜: {len(results)}")
        print(f"   - ìŒë†’ì´ ë²”ìœ„: {analysis_result['pitch_statistics']['min_pitch']:.1f}Hz - {analysis_result['pitch_statistics']['max_pitch']:.1f}Hz")
        print(f"   - í‰ê·  ìŒë†’ì´: {analysis_result['pitch_statistics']['avg_pitch']:.1f}Hz")
        
        return analysis_result
    
    def visualize_audio_analysis(self, analysis_result: Dict):
        """ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“Š ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”...")
        
        results = analysis_result['detailed_results']
        times = [r['time'] for r in results]
        pitches = [r['pitch'] for r in results]
        volumes = [r['volume'] for r in results]
        confidences = [r['confidence'] for r in results]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(3, 1, figsize=(15, 10))
        
        # ìŒë†’ì´ ê·¸ë˜í”„
        axes[0].plot(times, pitches, 'b-', alpha=0.7)
        axes[0].set_title('ìŒë†’ì´ ë³€í™” (Hz)')
        axes[0].set_ylabel('ì£¼íŒŒìˆ˜ (Hz)')
        axes[0].grid(True, alpha=0.3)
        
        # ë³¼ë¥¨ ê·¸ë˜í”„
        axes[1].plot(times, volumes, 'r-', alpha=0.7)
        axes[1].set_title('ë³¼ë¥¨ ë³€í™”')
        axes[1].set_ylabel('ë³¼ë¥¨')
        axes[1].grid(True, alpha=0.3)
        
        # ì‹ ë¢°ë„ ê·¸ë˜í”„
        axes[2].plot(times, confidences, 'g-', alpha=0.7)
        axes[2].set_title('ìŒë†’ì´ ê²€ì¶œ ì‹ ë¢°ë„')
        axes[2].set_ylabel('ì‹ ë¢°ë„')
        axes[2].set_xlabel('ì‹œê°„ (ì´ˆ)')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('audio_analysis_result.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ì‹œê°í™” ì™„ë£Œ: audio_analysis_result.png")
    
    def save_analysis_result(self, analysis_result: Dict, filename: str = 'audio_analysis.json'):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # NumPy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj
        
        serializable_result = convert_numpy_types(analysis_result)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸµ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    audio_file = "ì£¼ í’ˆì— í’ˆìœ¼ì†Œì„œ.mp3"
    
    try:
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = RealTimeAudioProcessor(audio_file)
        
        # ì „ì²´ ì˜¤ë””ì˜¤ ë¶„ì„
        analysis_result = processor.analyze_full_audio()
        
        # ê²°ê³¼ ì‹œê°í™”
        processor.visualize_audio_analysis(analysis_result)
        
        # ê²°ê³¼ ì €ì¥
        processor.save_analysis_result(analysis_result)
        
        # ì‹¤ì‹œê°„ ì¬ìƒ ì˜µì…˜
        print("\nğŸµ ì‹¤ì‹œê°„ ì¬ìƒì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
        choice = input().lower()
        
        if choice == 'y':
            print("ì‹¤ì‹œê°„ ì¬ìƒ ì‹œì‘... (Ctrl+Cë¡œ ì¤‘ì§€)")
            try:
                audio_thread = processor.play_audio_realtime()
                audio_thread.join()
            except KeyboardInterrupt:
                processor.stop_audio()
                print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
