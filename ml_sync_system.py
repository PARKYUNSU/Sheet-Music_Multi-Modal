"""
ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì•…ë³´-ì—°ì£¼ ì‹±í¬ ì‹œìŠ¤í…œ
ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë§¤ì¹­ì„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ëŒ€ì²´í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
"""

import json
import numpy as np
import time
import matplotlib.pyplot as plt
from typing import List, Dict, Optional, Tuple
from collections import deque
import librosa
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib
from improved_omr import ImprovedOMR
from real_time_audio_processor import RealTimeAudioProcessor

class MLSyncSystem:
    def __init__(self, sheet_music_path: str, audio_file_path: str):
        """
        ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹±í¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            sheet_music_path: ì•…ë³´ ì´ë¯¸ì§€ ê²½ë¡œ
            audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        self.sheet_music_path = sheet_music_path
        self.audio_file_path = audio_file_path
        
        # OMR ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ¼ ML ê¸°ë°˜ OMR ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.omr = ImprovedOMR()
        self.sheet_music_data = None
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸµ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.audio_processor = RealTimeAudioProcessor(audio_file_path)
        
        # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤
        self.models = {
            'sync_classifier': None,  # ë™ê¸°í™” ì—¬ë¶€ ë¶„ë¥˜
            'position_regressor': None,  # ìœ„ì¹˜ ì˜ˆì¸¡ íšŒê·€
            'confidence_regressor': None,  # ì‹ ë¢°ë„ ì˜ˆì¸¡ íšŒê·€
            'section_classifier': None  # êµ¬ê°„ ë¶„ë¥˜
        }
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ëŸ¬
        self.scalers = {
            'audio_features': StandardScaler(),
            'sheet_features': StandardScaler(),
            'combined_features': StandardScaler()
        }
        
        # í›ˆë ¨ ë°ì´í„°
        self.training_data = {
            'features': [],
            'labels': [],
            'positions': [],
            'confidences': [],
            'sections': []
        }
        
        # ë™ê¸°í™” ìƒíƒœ
        self.sync_state = {
            'current_position': 0,
            'sync_confidence': 0.0,
            'is_synced': False,
            'current_section': 'unknown',
            'ml_predictions': deque(maxlen=10)  # ìµœê·¼ 10ê°œ ì˜ˆì¸¡ ê²°ê³¼
        }
        
        print("âœ… ML ê¸°ë°˜ ì•…ë³´-ì—°ì£¼ ì‹±í¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_audio_features(self, audio_chunk: np.ndarray, sample_rate: int = 22050) -> Dict:
        """ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # 1. ê¸°ë³¸ ìŒë†’ì´ íŠ¹ì„±
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio_chunk, fmin=80, fmax=1000, sr=sample_rate
        )
        
        valid_f0 = f0[~np.isnan(f0)]
        if len(valid_f0) > 0:
            features['pitch_mean'] = np.mean(valid_f0)
            features['pitch_std'] = np.std(valid_f0)
            features['pitch_min'] = np.min(valid_f0)
            features['pitch_max'] = np.max(valid_f0)
            features['pitch_range'] = features['pitch_max'] - features['pitch_min']
        else:
            features.update({
                'pitch_mean': 0, 'pitch_std': 0, 'pitch_min': 0,
                'pitch_max': 0, 'pitch_range': 0
            })
        
        # 2. ë³¼ë¥¨ íŠ¹ì„±
        rms = librosa.feature.rms(y=audio_chunk)[0]
        features['volume_mean'] = np.mean(rms)
        features['volume_std'] = np.std(rms)
        features['volume_max'] = np.max(rms)
        
        # 3. ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„±
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_chunk, sr=sample_rate)[0]
        features['spectral_centroid_mean'] = np.mean(spectral_centroids)
        features['spectral_centroid_std'] = np.std(spectral_centroids)
        
        # 4. MFCC íŠ¹ì„± (ìŒìƒ‰ íŠ¹ì„±)
        mfccs = librosa.feature.mfcc(y=audio_chunk, sr=sample_rate, n_mfcc=13)
        for i in range(13):
            features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])
            features[f'mfcc_{i}_std'] = np.std(mfccs[i])
        
        # 5. ë¦¬ë“¬ íŠ¹ì„±
        onset_frames = librosa.onset.onset_detect(y=audio_chunk, sr=sample_rate)
        features['onset_count'] = len(onset_frames)
        if len(onset_frames) > 1:
            onset_intervals = np.diff(onset_frames)
            features['onset_interval_mean'] = np.mean(onset_intervals)
            features['onset_interval_std'] = np.std(onset_intervals)
        else:
            features['onset_interval_mean'] = 0
            features['onset_interval_std'] = 0
        
        # 6. í…œí¬ íŠ¹ì„±
        tempo, beats = librosa.beat.beat_track(y=audio_chunk, sr=sample_rate)
        features['tempo'] = tempo
        features['beat_count'] = len(beats)
        
        return features
    
    def extract_sheet_features(self, note: Dict, context_notes: List[Dict]) -> Dict:
        """ì•…ë³´ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # 1. ìŒí‘œ ê¸°ë³¸ íŠ¹ì„±
        note_freq = self.note_name_to_frequency(note['pitch'])
        features['note_frequency'] = note_freq
        features['note_duration'] = note.get('duration', 1.0)
        features['note_area'] = note.get('area', 0)
        features['note_aspect_ratio'] = note.get('aspect_ratio', 1.0)
        
        # 2. ìŒí‘œ íƒ€ì… íŠ¹ì„± (ì›í•« ì¸ì½”ë”©)
        note_types = ['whole', 'half', 'quarter', 'eighth', 'sixteenth']
        for note_type in note_types:
            features[f'note_type_{note_type}'] = 1 if note.get('type') == note_type else 0
        
        # 3. ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„± (ì£¼ë³€ ìŒí‘œë“¤)
        if len(context_notes) > 0:
            context_freqs = [self.note_name_to_frequency(n['pitch']) for n in context_notes]
            context_freqs = [f for f in context_freqs if f > 0]
            
            if context_freqs:
                features['context_pitch_mean'] = np.mean(context_freqs)
                features['context_pitch_std'] = np.std(context_freqs)
                features['context_pitch_range'] = np.max(context_freqs) - np.min(context_freqs)
            else:
                features.update({
                    'context_pitch_mean': 0, 'context_pitch_std': 0, 'context_pitch_range': 0
                })
            
            # ìŒì • ê´€ê³„
            if len(context_freqs) > 1:
                intervals = []
                for i in range(1, len(context_freqs)):
                    interval = 1200 * np.log2(context_freqs[i] / context_freqs[i-1])
                    intervals.append(interval)
                features['interval_mean'] = np.mean(intervals)
                features['interval_std'] = np.std(intervals)
            else:
                features['interval_mean'] = 0
                features['interval_std'] = 0
        
        # 4. ìœ„ì¹˜ íŠ¹ì„±
        features['note_position'] = note.get('index', 0)
        features['relative_position'] = note.get('index', 0) / max(1, len(context_notes))
        
        return features
    
    def create_training_data(self):
        """í›ˆë ¨ ë°ì´í„° ìƒì„±"""
        print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ì•…ë³´ ë°ì´í„° ë¡œë“œ
        self.sheet_music_data = self.omr.process_sheet_music_improved(self.sheet_music_path)
        
        # ì˜¤ë””ì˜¤ ë¶„ì„
        audio_analysis = self.audio_processor.analyze_full_audio()
        audio_results = audio_analysis['detailed_results']
        
        # ì‹œê°„ì¶• ë³€í™˜
        self.convert_notes_to_timeline()
        
        notes = self.sheet_music_data['notes']
        
        # ê° ì˜¤ë””ì˜¤ ìƒ˜í”Œì— ëŒ€í•´ í›ˆë ¨ ë°ì´í„° ìƒì„±
        for i, audio_result in enumerate(audio_results):
            current_time = audio_result['time']
            current_pitch = audio_result['pitch']
            
            # ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ
            # ì‹¤ì œë¡œëŠ” audio_chunkê°€ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê²°ê³¼ë¥¼ ì‚¬ìš©
            audio_features = {
                'pitch_mean': current_pitch,
                'pitch_std': 0,  # ë‹¨ì¼ ìƒ˜í”Œì´ë¯€ë¡œ 0
                'volume_mean': audio_result['volume'],
                'volume_std': 0,
                'tempo': 120,  # ê¸°ë³¸ê°’
                'onset_count': 0,
                'spectral_centroid_mean': current_pitch * 2,  # ê·¼ì‚¬ê°’
            }
            
            # ê° ì•…ë³´ ìŒí‘œì™€ì˜ ë§¤ì¹­ ì‹œë„
            for j, note in enumerate(notes):
                # ì•…ë³´ íŠ¹ì„± ì¶”ì¶œ
                context_notes = notes[max(0, j-2):min(len(notes), j+3)]
                sheet_features = self.extract_sheet_features(note, context_notes)
                
                # ê²°í•©ëœ íŠ¹ì„±
                combined_features = {**audio_features, **sheet_features}
                
                # ë¼ë²¨ ìƒì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì‹œì‘)
                time_diff = abs(current_time - note['start_time'])
                pitch_diff = abs(current_pitch - sheet_features['note_frequency'])
                
                # ë§¤ì¹­ ë¼ë²¨ (ì„ê³„ê°’ ê¸°ë°˜)
                is_match = (time_diff < 5.0 and pitch_diff < 100)  # 5ì´ˆ, 100Hz í—ˆìš©
                confidence = max(0, 1.0 - (time_diff / 5.0) - (pitch_diff / 100))
                
                # êµ¬ê°„ ë¼ë²¨ (ì‹œê°„ ê¸°ë°˜)
                if current_time < 10:
                    section = 'intro'
                elif current_time < 130:
                    section = 'verse1'
                else:
                    section = 'verse2'
                
                # í›ˆë ¨ ë°ì´í„°ì— ì¶”ê°€
                self.training_data['features'].append(combined_features)
                self.training_data['labels'].append(1 if is_match else 0)
                self.training_data['positions'].append(j)
                self.training_data['confidences'].append(confidence)
                self.training_data['sections'].append(section)
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.training_data['features'])}ê°œ ìƒ˜í”Œ")
    
    def prepare_ml_features(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ íŠ¹ì„± ì¤€ë¹„"""
        if not self.training_data['features']:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. create_training_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜
        feature_names = list(self.training_data['features'][0].keys())
        X = np.array([[sample[feature] for feature in feature_names] 
                     for sample in self.training_data['features']])
        
        # ë¼ë²¨ë“¤
        y_sync = np.array(self.training_data['labels'])
        y_positions = np.array(self.training_data['positions'])
        y_confidences = np.array(self.training_data['confidences'])
        y_sections = np.array(self.training_data['sections'])
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scalers['combined_features'].fit_transform(X)
        
        return X_scaled, y_sync, y_positions, y_confidences, y_sections
    
    def train_models(self):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # íŠ¹ì„± ì¤€ë¹„
        X, y_sync, y_positions, y_confidences, y_sections = self.prepare_ml_features()
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_sync_train, y_sync_test = train_test_split(
            X, y_sync, test_size=0.2, random_state=42, stratify=y_sync
        )
        
        # 1. ë™ê¸°í™” ë¶„ë¥˜ê¸° í›ˆë ¨
        print("   - ë™ê¸°í™” ë¶„ë¥˜ê¸° í›ˆë ¨...")
        self.models['sync_classifier'] = RandomForestClassifier(
            n_estimators=100, random_state=42, class_weight='balanced'
        )
        self.models['sync_classifier'].fit(X_train, y_sync_train)
        
        # ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€
        y_sync_pred = self.models['sync_classifier'].predict(X_test)
        sync_accuracy = accuracy_score(y_sync_test, y_sync_pred)
        print(f"     ë™ê¸°í™” ë¶„ë¥˜ ì •í™•ë„: {sync_accuracy:.3f}")
        
        # 2. ìœ„ì¹˜ íšŒê·€ê¸° í›ˆë ¨ (ë™ê¸°í™”ëœ ìƒ˜í”Œë§Œ)
        print("   - ìœ„ì¹˜ íšŒê·€ê¸° í›ˆë ¨...")
        sync_mask = y_sync == 1
        if np.sum(sync_mask) > 10:  # ì¶©ë¶„í•œ ì–‘ì„± ìƒ˜í”Œì´ ìˆëŠ” ê²½ìš°
            X_sync = X[sync_mask]
            y_pos_sync = y_positions[sync_mask]
            
            X_train_sync, X_test_sync, y_pos_train, y_pos_test = train_test_split(
                X_sync, y_pos_sync, test_size=0.2, random_state=42
            )
            
            self.models['position_regressor'] = RandomForestRegressor(
                n_estimators=100, random_state=42
            )
            self.models['position_regressor'].fit(X_train_sync, y_pos_train)
            
            y_pos_pred = self.models['position_regressor'].predict(X_test_sync)
            pos_mse = np.mean((y_pos_test - y_pos_pred) ** 2)
            print(f"     ìœ„ì¹˜ íšŒê·€ MSE: {pos_mse:.3f}")
        
        # 3. ì‹ ë¢°ë„ íšŒê·€ê¸° í›ˆë ¨
        print("   - ì‹ ë¢°ë„ íšŒê·€ê¸° í›ˆë ¨...")
        # ì‹ ë¢°ë„ ë°ì´í„°ë„ ê°™ì€ ë¶„í•  ì‚¬ìš©
        _, _, y_conf_train, y_conf_test = train_test_split(
            X, y_confidences, test_size=0.2, random_state=42
        )
        
        self.models['confidence_regressor'] = RandomForestRegressor(
            n_estimators=100, random_state=42
        )
        self.models['confidence_regressor'].fit(X_train, y_conf_train)
        
        y_conf_pred = self.models['confidence_regressor'].predict(X_test)
        conf_mse = np.mean((y_conf_test - y_conf_pred) ** 2)
        print(f"     ì‹ ë¢°ë„ íšŒê·€ MSE: {conf_mse:.3f}")
        
        # 4. êµ¬ê°„ ë¶„ë¥˜ê¸° í›ˆë ¨
        print("   - êµ¬ê°„ ë¶„ë¥˜ê¸° í›ˆë ¨...")
        section_encoder = {'intro': 0, 'verse1': 1, 'verse2': 2}
        y_sections_encoded = np.array([section_encoder.get(s, 0) for s in y_sections])
        
        X_train_sec, X_test_sec, y_sec_train, y_sec_test = train_test_split(
            X, y_sections_encoded, test_size=0.2, random_state=42
        )
        
        self.models['section_classifier'] = RandomForestClassifier(
            n_estimators=100, random_state=42
        )
        self.models['section_classifier'].fit(X_train_sec, y_sec_train)
        
        y_sec_pred = self.models['section_classifier'].predict(X_test_sec)
        sec_accuracy = accuracy_score(y_sec_test, y_sec_pred)
        print(f"     êµ¬ê°„ ë¶„ë¥˜ ì •í™•ë„: {sec_accuracy:.3f}")
        
        print("âœ… ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
    
    def predict_sync(self, audio_features: Dict, sheet_features: Dict) -> Dict:
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë™ê¸°í™” ì˜ˆì¸¡"""
        # íŠ¹ì„± ê²°í•©
        combined_features = {**audio_features, **sheet_features}
        
        # íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜
        feature_names = list(combined_features.keys())
        X = np.array([[combined_features[feature] for feature in feature_names]])
        
        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scalers['combined_features'].transform(X)
        
        # ì˜ˆì¸¡
        predictions = {}
        
        if self.models['sync_classifier']:
            sync_prob = self.models['sync_classifier'].predict_proba(X_scaled)[0]
            predictions['is_sync'] = sync_prob[1] > 0.5
            predictions['sync_confidence'] = sync_prob[1]
        
        if self.models['position_regressor'] and predictions.get('is_sync'):
            position_pred = self.models['position_regressor'].predict(X_scaled)[0]
            predictions['predicted_position'] = int(round(position_pred))
        
        if self.models['confidence_regressor']:
            confidence_pred = self.models['confidence_regressor'].predict(X_scaled)[0]
            predictions['ml_confidence'] = max(0, min(1, confidence_pred))
        
        if self.models['section_classifier']:
            section_pred = self.models['section_classifier'].predict(X_scaled)[0]
            section_names = ['intro', 'verse1', 'verse2']
            predictions['predicted_section'] = section_names[section_pred]
        
        return predictions
    
    def ml_sync_monitoring(self, duration: float = 60.0):
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë™ê¸°í™” ëª¨ë‹ˆí„°ë§"""
        print(f"ğŸ¤– ML ê¸°ë°˜ ë™ê¸°í™” ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ìµœëŒ€ {duration}ì´ˆ)...")
        
        # í›ˆë ¨ ë°ì´í„° ìƒì„± ë° ëª¨ë¸ í›ˆë ¨
        self.create_training_data()
        self.train_models()
        
        # ì˜¤ë””ì˜¤ ë¶„ì„
        audio_analysis = self.audio_processor.analyze_full_audio()
        audio_results = audio_analysis['detailed_results']
        
        notes = self.sheet_music_data['notes']
        sync_results = []
        
        for result in audio_results[:int(duration * 10)]:
            current_time = result['time']
            current_pitch = result['pitch']
            
            # ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ
            audio_features = {
                'pitch_mean': current_pitch,
                'pitch_std': 0,
                'volume_mean': result['volume'],
                'volume_std': 0,
                'tempo': 120,
                'onset_count': 0,
                'spectral_centroid_mean': current_pitch * 2,
            }
            
            best_match = None
            best_score = 0.0
            
            # ê° ì•…ë³´ ìŒí‘œì™€ ML ë§¤ì¹­ ì‹œë„
            for note in notes:
                context_notes = notes[max(0, note['index']-2):min(len(notes), note['index']+3)]
                sheet_features = self.extract_sheet_features(note, context_notes)
                
                # ML ì˜ˆì¸¡
                predictions = self.predict_sync(audio_features, sheet_features)
                
                if predictions.get('is_sync') and predictions.get('sync_confidence', 0) > best_score:
                    best_score = predictions['sync_confidence']
                    best_match = {
                        'note': note,
                        'predictions': predictions
                    }
            
            # ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
            if best_match and best_score > 0.5:
                self.sync_state['current_position'] = best_match['note']['index']
                self.sync_state['sync_confidence'] = best_score
                self.sync_state['is_synced'] = True
                self.sync_state['current_section'] = best_match['predictions'].get('predicted_section', 'unknown')
            else:
                self.sync_state['sync_confidence'] = best_score
                self.sync_state['is_synced'] = False
            
            # ê²°ê³¼ ì €ì¥
            sync_results.append({
                'time': current_time,
                'audio_pitch': current_pitch,
                'best_match': best_match,
                'sync_state': self.sync_state.copy()
            })
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if len(sync_results) % 100 == 0:
                section = self.sync_state['current_section']
                print(f"â±ï¸ {current_time:.1f}ì´ˆ [{section}]: "
                      f"ìŒë†’ì´={current_pitch:.1f}Hz, "
                      f"MLì‹ ë¢°ë„={best_score:.3f}, "
                      f"ë™ê¸°í™”={'âœ…' if self.sync_state['is_synced'] else 'âŒ'}")
        
        return sync_results
    
    def save_models(self, model_dir: str = 'ml_models'):
        """í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥"""
        import os
        os.makedirs(model_dir, exist_ok=True)
        
        for model_name, model in self.models.items():
            if model is not None:
                joblib.dump(model, f'{model_dir}/{model_name}.pkl')
        
        for scaler_name, scaler in self.scalers.items():
            joblib.dump(scaler, f'{model_dir}/{scaler_name}.pkl')
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_dir}/")
    
    def load_models(self, model_dir: str = 'ml_models'):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        import os
        if not os.path.exists(model_dir):
            print(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
            return False
        
        for model_name in self.models.keys():
            model_path = f'{model_dir}/{model_name}.pkl'
            if os.path.exists(model_path):
                self.models[model_name] = joblib.load(model_path)
        
        for scaler_name in self.scalers.keys():
            scaler_path = f'{model_dir}/{scaler_name}.pkl'
            if os.path.exists(scaler_path):
                self.scalers[scaler_name] = joblib.load(scaler_path)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_dir}/")
        return True
    
    def convert_notes_to_timeline(self):
        """ìŒí‘œ ë°ì´í„°ë¥¼ ì‹œê°„ì¶•ìœ¼ë¡œ ë³€í™˜"""
        if not self.sheet_music_data or 'notes' not in self.sheet_music_data:
            return
        
        notes = self.sheet_music_data['notes']
        tempo = 120.0  # ê¸°ë³¸ í…œí¬
        
        current_time = 0.0
        for i, note in enumerate(notes):
            note['start_time'] = current_time
            note_duration = note.get('duration', 1.0)
            note['end_time'] = current_time + (note_duration * 60.0 / tempo)
            current_time = note['end_time']
            note['index'] = i
    
    def note_name_to_frequency(self, note_name: str) -> float:
        """ìŒí‘œëª…ì„ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜"""
        if note_name == "Silence":
            return 0.0
        
        note_map = {
            'C': -9, 'C#': -8, 'D': -7, 'D#': -6, 'E': -5, 'F': -4,
            'F#': -3, 'G': -2, 'G#': -1, 'A': 0, 'A#': 1, 'B': 2
        }
        
        try:
            note = note_name[:-1]
            octave = int(note_name[-1])
            semitones = note_map[note] + (octave - 4) * 12
            frequency = 440.0 * (2 ** (semitones / 12))
            return frequency
        except:
            return 0.0
    
    def visualize_ml_sync_results(self, sync_results: List[Dict]):
        """ML ë™ê¸°í™” ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“Š ML ë™ê¸°í™” ê²°ê³¼ ì‹œê°í™”...")
        
        times = [r['time'] for r in sync_results]
        audio_pitches = [r['audio_pitch'] for r in sync_results]
        ml_confidences = [r['sync_state']['sync_confidence'] for r in sync_results]
        sync_status = [1 if r['sync_state']['is_synced'] else 0 for r in sync_results]
        sections = [r['sync_state']['current_section'] for r in sync_results]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(4, 1, figsize=(15, 16))
        
        # 1. ì˜¤ë””ì˜¤ ìŒë†’ì´
        axes[0].plot(times, audio_pitches, 'b-', alpha=0.7, label='Audio Pitch')
        axes[0].set_title('Audio Pitch Over Time (ML Enhanced)')
        axes[0].set_ylabel('Frequency (Hz)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. ML ì‹ ë¢°ë„
        axes[1].plot(times, ml_confidences, 'g-', alpha=0.7)
        axes[1].axhline(y=0.5, color='orange', linestyle='--', label='ML Threshold')
        axes[1].set_title('ML Sync Confidence')
        axes[1].set_ylabel('Confidence')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. êµ¬ê°„ ë¶„ë¥˜
        section_colors = {'intro': 'red', 'verse1': 'blue', 'verse2': 'green', 'unknown': 'gray'}
        for i, section in enumerate(sections):
            if section in section_colors:
                axes[2].scatter(times[i], 1, c=section_colors[section], s=10, alpha=0.7)
        axes[2].set_title('ML Section Classification')
        axes[2].set_ylabel('Section')
        axes[2].set_ylim(0.5, 1.5)
        axes[2].grid(True, alpha=0.3)
        
        # 4. ë™ê¸°í™” ìƒíƒœ
        axes[3].plot(times, sync_status, 'r-', alpha=0.7)
        axes[3].set_title('ML Sync Status')
        axes[3].set_ylabel('Synced (1/0)')
        axes[3].set_xlabel('Time (seconds)')
        axes[3].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('ml_sync_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… ML ì‹œê°í™” ì™„ë£Œ: ml_sync_analysis.png")
    
    def get_ml_sync_statistics(self, sync_results: List[Dict]) -> Dict:
        """ML ë™ê¸°í™” í†µê³„ ê³„ì‚°"""
        total_samples = len(sync_results)
        synced_samples = sum(1 for r in sync_results if r['sync_state']['is_synced'])
        
        avg_confidence = np.mean([r['sync_state']['sync_confidence'] for r in sync_results])
        max_confidence = max([r['sync_state']['sync_confidence'] for r in sync_results])
        
        # êµ¬ê°„ë³„ í†µê³„
        section_stats = {}
        for section in set(r['sync_state']['current_section'] for r in sync_results):
            section_results = [r for r in sync_results if r['sync_state']['current_section'] == section]
            section_synced = sum(1 for r in section_results if r['sync_state']['is_synced'])
            section_stats[section] = {
                'total_samples': len(section_results),
                'synced_samples': section_synced,
                'sync_rate': section_synced / len(section_results) if section_results else 0,
                'avg_confidence': np.mean([r['sync_state']['sync_confidence'] for r in section_results])
            }
        
        return {
            'total_samples': total_samples,
            'synced_samples': synced_samples,
            'overall_sync_rate': synced_samples / total_samples if total_samples > 0 else 0,
            'avg_ml_confidence': avg_confidence,
            'max_ml_confidence': max_confidence,
            'section_statistics': section_stats
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì•…ë³´-ì—°ì£¼ ì‹±í¬ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # íŒŒì¼ ê²½ë¡œ
    sheet_music_path = "sheet music_1.png"
    audio_file_path = "ì£¼ í’ˆì— í’ˆìœ¼ì†Œì„œ.mp3"
    
    try:
        # ML ì‹±í¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        ml_sync = MLSyncSystem(sheet_music_path, audio_file_path)
        
        # ML ê¸°ë°˜ ë™ê¸°í™” ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        sync_results = ml_sync.ml_sync_monitoring(duration=60.0)
        
        # í†µê³„ ê³„ì‚° ë° ì¶œë ¥
        stats = ml_sync.get_ml_sync_statistics(sync_results)
        print(f"\nğŸ“Š ML ë™ê¸°í™” í†µê³„:")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {stats['total_samples']}")
        print(f"   - ë™ê¸°í™”ëœ ìƒ˜í”Œ: {stats['synced_samples']}")
        print(f"   - ì „ì²´ ë™ê¸°í™”ìœ¨: {stats['overall_sync_rate']:.2%}")
        print(f"   - í‰ê·  ML ì‹ ë¢°ë„: {stats['avg_ml_confidence']:.3f}")
        print(f"   - ìµœëŒ€ ML ì‹ ë¢°ë„: {stats['max_ml_confidence']:.3f}")
        
        print(f"\nğŸ“ˆ êµ¬ê°„ë³„ ML ë™ê¸°í™”ìœ¨:")
        for section, section_stats in stats['section_statistics'].items():
            print(f"   - {section}: {section_stats['sync_rate']:.2%} ({section_stats['synced_samples']}/{section_stats['total_samples']})")
        
        # ê²°ê³¼ ì‹œê°í™”
        ml_sync.visualize_ml_sync_results(sync_results)
        
        # ëª¨ë¸ ì €ì¥
        ml_sync.save_models()
        
        print("\nâœ… ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì•…ë³´-ì—°ì£¼ ì‹±í¬ ì‹œìŠ¤í…œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
